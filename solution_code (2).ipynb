{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1A\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/movie_metadata.csv')\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1B\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/movie_metadata.csv')\n",
    "limitDF = df[['imdb_score','gross', 'budget', 'duration', 'director_facebook_likes', 'num_voted_users']]\n",
    "limitDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1C\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/movie_metadata.csv')\n",
    "limitDF = df[['imdb_score','gross', 'budget', 'duration', 'director_facebook_likes', 'num_voted_users']]\n",
    "limitDF.dropna(axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1D\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/movie_metadata.csv')\n",
    "limitDF = df[['imdb_score','gross', 'budget', 'duration', 'director_facebook_likes', 'num_voted_users']]\n",
    "notnulldf = limitDF.dropna(axis='rows')\n",
    "notnulldf.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1E\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/movie_metadata.csv')\n",
    "x = df['imdb_score']\n",
    "\n",
    "ax = plt.subplots(figsize=(12, 6))\n",
    "plt.hist(x, bins = 20, edgecolor = 'k')\n",
    "plt.xlabel ('IMDb score (Out of 10)', fontsize =14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.title(\"Histogram of the Movie Scores\", fontsize=16)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/movie_metadata.csv')\n",
    "movie_count = df['title_year'].value_counts()\n",
    "movie_count2 = movie_count[movie_count >= 15]\n",
    "median = df.loc[df['title_year'].isin(movie_count2.index)].groupby('title_year')['imdb_score'].median()\n",
    "ax = plt.subplots(figsize=(10, 5))\n",
    "plt.plot(median, '-sg', markersize='4')\n",
    "plt.title('IMDB Ratings by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('IMDB Rating')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1G\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/movie_metadata.csv')\n",
    "df1 = df[['plot_keywords', 'imdb_score']]\n",
    "df2 = df1.assign(plot_keywords=df1['plot_keywords'].str.split('|')).explode('plot_keywords')\n",
    "df5 = df2['plot_keywords'].value_counts()\n",
    "df6 = df5[df5 >= 50]\n",
    "mean = df2.loc[df2['plot_keywords'].isin(df6.index)].groupby('plot_keywords')['imdb_score'].mean()\n",
    "mean1 = mean.sort_values()\n",
    "mean1\n",
    "ax = mean1.plot(kind='barh', figsize=(5,10), color='#9719b0', width=0.7)\n",
    "plt.title('Ranking of Plot Keywords')\n",
    "plt.xlabel('IMDB Score')\n",
    "plt.ylabel('Keyword')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1H\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/movie_metadata.csv')\n",
    "director_count = df['director_name'].value_counts()\n",
    "director_count2 = director_count[director_count >= 10]\n",
    "mean = df.loc[df['director_name'].isin(director_count2.index)].groupby('director_name')['imdb_score'].mean()\n",
    "mean2 = df.loc[df['director_name'].isin(director_count2.index)].groupby('director_name')['budget'].mean()\n",
    "means=mean.sort_values()\n",
    "means2=mean2.sort_values()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "means.plot(x=['imdb_score'],y=['director_name'], kind='barh', figsize=(10,5))\n",
    "plt.yticks(fontsize=8)\n",
    "plt.ylabel('Director')\n",
    "plt.xlabel('Average IMDB Score')\n",
    "plt.title(\"Comparing Director's (With Minimum of 10 Films) Average Budget and Average Movie Rating\")\n",
    "\n",
    "fig,axs=plt.subplots()\n",
    "means2.plot(x=['budget'],y=['director_name'], kind='barh', figsize=(10,5), color='r')\n",
    "plt.yticks(fontsize=8)\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Director')\n",
    "plt.xlabel('Average Budget')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two bar charts show which directors over or underperform when comparing their budget to IMDB score. Richard Linklater for example is a massive overperformer as he has the lowest average budget yet is ranked 6th best in average score whilst Michael Bay massively underperforms as he is has the second largest average budget yet is only 26th overall in average score. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2A\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/football_E0_2021.csv')\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2B\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/football_E0_2021.csv')\n",
    "Teams = df['HomeTeam'].to_numpy()\n",
    "\n",
    "PLTeams = np.unique(Teams)\n",
    "PLTeams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2C\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/football_E0_2021.csv')\n",
    "\n",
    "home = df.groupby('HomeTeam').size()\n",
    "away = df.groupby('AwayTeam').size()\n",
    "total = away + home\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2D\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/football_E0_2021.csv')\n",
    "def calcScore(row, w=3, l=0, d=1):\n",
    "        if row['FTHG'] > row['FTAG']:\n",
    "            home= w\n",
    "            away= l\n",
    "        elif row['FTHG'] == row['FTAG']:\n",
    "            home=d\n",
    "            away=d\n",
    "        elif row['FTHG'] < row['FTAG']:\n",
    "            home=l\n",
    "            away=w\n",
    "        return home,away\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2E\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/football_E0_2021.csv')\n",
    "df['HomeP'] = df.apply(lambda x: calcScore(x)[0],axis=1)\n",
    "df['AwayP'] = df.apply(lambda x: calcScore(x)[1],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/football_E0_2021.csv')\n",
    "df['HomeP'] = df.apply(lambda x: calcScore(x)[0], axis =1)\n",
    "df['AwayP'] = df.apply(lambda x: calcScore(x)[1],axis =1)\n",
    "def TeamPoints(df, team, H_A):\n",
    "    if(H_A == 'Away'):\n",
    "        df =df[df['AwayTeam']==team]\n",
    "        points =df['AwayP'].sum()\n",
    "    else:\n",
    "        df = df[df['HomeTeam']==team]\n",
    "        points = df['HomeP'].sum()\n",
    "    return points\n",
    "print((TeamPoints(df,'Man City', 'Away')),(TeamPoints(df,'Man City', 'Home')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2G\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/football_E0_2021.csv')\n",
    "df['HomeP'] = df.apply(lambda x: calcScore(x)[0], axis =1)\n",
    "df['AwayP'] = df.apply(lambda x: calcScore(x)[1],axis =1)\n",
    "def TeamsPointsDict(df, PLTeams):\n",
    "    dict={}\n",
    "    for team in PLTeams:\n",
    "        p = TeamPoints(df,team,'Away')+TeamPoints(df,team,'Home')\n",
    "        dict[team] = p\n",
    "    return dict\n",
    "\n",
    "TeamsPointsDict(df,PLTeams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2H\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/football_E0_2021.csv')\n",
    "df['HomeP'] = df.apply(lambda x: calcScore(x)[0], axis =1)\n",
    "df['AwayP'] = df.apply(lambda x: calcScore(x)[1],axis =1)\n",
    "table = pd.DataFrame(TeamsPointsDict(df,PLTeams).items())\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2I\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/football_E0_2021.csv')\n",
    "df['HomeP'] = df.apply(lambda x: calcScore(x)[0], axis =1)\n",
    "df['AwayP'] = df.apply(lambda x: calcScore(x)[1],axis =1)\n",
    "table.columns=['Team','Points']\n",
    "table1 = table.sort_values(by='Points', ascending=False)\n",
    "table2 = table1.reset_index(drop=True)\n",
    "table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2J\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/football_E0_2021.csv')\n",
    "df['HomeP'] = df.apply(lambda x: calcScore(x)[0], axis =1)\n",
    "df['AwayP'] = df.apply(lambda x: calcScore(x)[1],axis =1)\n",
    "table = pd.DataFrame(TeamsPointsDict(df,PLTeams).items())\n",
    "\n",
    "def GetTable(df, w=3,l=0,d=1):\n",
    "    dict={TeamsPointsDict(df,PLTeams)}\n",
    "    table=pd.DataFrame(dict.items())\n",
    "    table.columns = ['Team','Total Points']\n",
    "    table1= table.sort_values(by='Total Points', ascending=False)\n",
    "    \n",
    "print(GetTable(df))\n",
    "#?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/football_E0_2021.csv')\n",
    "\n",
    "df['HomeP'] = df.apply(lambda x: calcScore(x)[0], axis =1)\n",
    "df['AwayP'] = df.apply(lambda x: calcScore(x)[1],axis =1)\n",
    "\n",
    "def calcScore(row, w=2, l=0, d=1):\n",
    "        if (row['FTHG'] > row['FTAG']):\n",
    "            hscore= w\n",
    "            ascore= l\n",
    "        elif (row['FTHG'] == row['FTAG']):\n",
    "            hscore=d\n",
    "            ascore=d\n",
    "        elif (row['FTHG'] < row['FTAG']):\n",
    "            hscore=l\n",
    "            ascore=w\n",
    "        return hscore,ascore\n",
    "\n",
    "df['HomeP'] = df.apply(lambda x: calcScore(x)[0], axis =1)\n",
    "df['AwayP'] = df.apply(lambda x: calcScore(x)[1],axis =1)\n",
    "\n",
    "def TeamPoints(df, team, H_A):\n",
    "    if(H_A == 'Home'):\n",
    "        df =df.loc[df['HomeTeam']==team]\n",
    "        score =df['HomeP'].sum()\n",
    "    else:\n",
    "        df = df.loc[df['AwayTeam']==team]\n",
    "        score = df['AwayP'].sum()\n",
    "    return score    \n",
    "\n",
    "def GetTable(df, w=2,l=0,d=1):\n",
    "    dict=TeamsPointsDict(df,PLTeams)\n",
    "    table=pd.DataFrame(dict.items())\n",
    "    table.columns = ['Team','Total Points']\n",
    "    table2= table.sort_values(by='Total Points', ascending=False)\n",
    "    \n",
    "print(GetTable(df))\n",
    "\n",
    "#?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2L\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/football_E0_2021.csv')\n",
    "df1 = df.groupby('Referee')['HY'].mean()\n",
    "df2 = df.groupby('Referee')['AY'].mean()\n",
    "\n",
    "df3=df1.mean()\n",
    "df4=df2.mean()\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "df1.plot(x=['Referee'],y=['HY'], kind='bar', color='g')\n",
    "plt.ylabel('Average number of yellow card for Home team')\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "df2.plot(x=['Referee'],y=['AY'], kind='bar', color='y')\n",
    "plt.ylabel('Avergae number of yellow cards for Away team')\n",
    "print('Average yellow cards for Home team is',df3,'and the average number of yellow cards for away teams is',df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two graphs show how many yellow cards different referee give to Home and Away team with the results showing that on average away teams recieve slightly more yellow cars (1.41 compared to 1.46). If this data was from a season where there were fans in the stadium I believe there would be more away yellow cards as the voice of the fans can influence the referees decsion making. The graphs also show which referees give the most yellow cards with Lee Mason giving the least and Stuart Attwell giving the most. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3A\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/owid-covid-data.csv')\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3B\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/owid-covid-data.csv')\n",
    "date = df['date'].to_numpy()\n",
    "uniquedate = np.unique(date)\n",
    "len(uniquedate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3C\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/owid-covid-data.csv')\n",
    "countries = df['iso_code'].to_numpy()\n",
    "uniquecountry = np.unique(countries)\n",
    "len(uniquecountry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/owid-covid-data.csv')\n",
    "new_df = df[~df.iso_code.str.contains('OWID_')]\n",
    "countries = new_df['iso_code'].to_numpy()\n",
    "uniquecountry = np.unique(countries)\n",
    "df = df.reset_index(drop=True)\n",
    "len(uniquecountry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3E\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/owid-covid-data.csv')\n",
    "df.date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/owid-covid-data.csv')\n",
    "df['total_cases_per_100'] = df['total_cases_per_million']/10000\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3G DaysSince1Jan20\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/owid-covid-data.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "Dayssince = pd.Timestamp('2020-01-01')\n",
    "df['DaysSince1Jan20'] = (df['date'] - Dayssince).dt.days\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3H\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/owid-covid-data.csv')\n",
    "df1 = df.drop_duplicates(subset = (\"male_smokers\",'female_smokers'), keep = 'first')\n",
    "df_country1 = df1[['iso_code', 'male_smokers', 'female_smokers']]\n",
    "df_country = df_country1.groupby(['iso_code']).sum()\n",
    "df_country\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3I\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/owid-covid-data.csv')\n",
    "df1 = df.drop_duplicates(subset = (\"male_smokers\",'female_smokers'), keep = 'first')\n",
    "df_country1 = df1[['iso_code', 'male_smokers', 'female_smokers']]\n",
    "df_country = df_country1.groupby(['iso_code']).sum()\n",
    "df_country\n",
    "y = df_country['male_smokers']\n",
    "x = df_country['female_smokers']\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(x,y, alpha = 0.5, s=60)\n",
    "\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlim(0.07,60)\n",
    "plt.ylim(6,90)\n",
    "plt.title('Country-level Male vs. Female Smokers')\n",
    "plt.ylabel('Male Smokers')\n",
    "plt.xlabel('Female Smokers')\n",
    "\n",
    "plt.annotate('GBR', (20, 24.6), color='red',fontsize=8)\n",
    "plt.annotate('IND', (2, 20.5), color='red',fontsize=8)\n",
    "plt.annotate('BRA', (10,17.9), color='red',fontsize=8)\n",
    "plt.annotate('CHN', (2, 48.5), color='red',fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3J\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/owid-covid-data.csv')\n",
    "df_GBR = df[df[\"iso_code\"].isin(['GBR'])]\n",
    "df_GBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/owid-covid-data.csv')\n",
    "df_GBR = df[df[\"iso_code\"].isin(['GBR'])]\n",
    "df_GBR['total_cases_per_million_smoothed'] = df_GBR['total_cases_per_million'].rolling(5, center = True, min_periods = 1).mean()\n",
    "df_GBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3L\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/owid-covid-data.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "Dayssince = pd.Timestamp('2020-01-01')\n",
    "df['DaysSince1Jan20'] = (df['date'] - Dayssince).dt.days\n",
    "\n",
    "df_GBR = df[df[\"iso_code\"].isin(['GBR'])]\n",
    "df_USA = df[df[\"iso_code\"].isin(['USA'])]\n",
    "df_IND = df[df[\"iso_code\"].isin(['IND'])]\n",
    "df_IND1 = df_IND[(df_IND[['new_cases_per_million']] != 0 ).all(axis=1)]\n",
    "df_USA1 = df_USA[(df_USA[['new_cases_per_million']] != 0 ).all(axis=1)]\n",
    "df_GBR1 = df_GBR[(df_GBR[['new_cases_per_million']] != 0 ).all(axis=1)]\n",
    "\n",
    "df_GBR['total_cases_per_million_smoothed'] = df_GBR['total_cases_per_million'].rolling(5, center = True, min_periods = 1).mean()\n",
    "df_USA['total_cases_per_million_smoothed'] = df_USA['total_cases_per_million'].rolling(5, center = True, min_periods = 1).mean()\n",
    "df_IND['total_cases_per_million_smoothed'] = df_IND['total_cases_per_million'].rolling(5, center = True, min_periods = 1).mean()\n",
    "\n",
    "\n",
    "ax = df_GBR1.plot(y = 'new_cases_per_million', x='DaysSince1Jan20', style = 'k', lw=1, figsize=(15,5), label= 'GBR')\n",
    "df_GBR1.plot(y = 'new_cases_smoothed_per_million', x = 'DaysSince1Jan20', style= '--k', lw=1, label='', ax=ax)\n",
    "df_IND1.plot(y = 'new_cases_per_million', x='DaysSince1Jan20' ,style = '#06b202', lw=1, label='IND',  ax=ax)\n",
    "df_IND1.plot(y = 'new_cases_smoothed_per_million', x = 'DaysSince1Jan20', color = '#06b202', style='--', lw=1, label='', ax=ax)\n",
    "df_USA1.plot(y= 'new_cases_per_million', x='DaysSince1Jan20', style ='r',lw=1, label='USA', ax=ax)\n",
    "df_USA1.plot(y= 'new_cases_smoothed_per_million', x ='DaysSince1Jan20',style = '--r', lw=1, label='', ax=ax)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.vlines(x=[83,305,371], ymin=0, ymax=1400, color='#85827e', alpha=0.7)\n",
    "plt.text(83.2,0.001,'2020-03-23', color='#85827e', alpha=0.8)\n",
    "plt.text(305.2,0.001,'2020-11-01', color='#85827e',alpha=0.8)\n",
    "plt.text(371.2,0.001, '2020-01-05',color='#85827e',alpha=0.8)\n",
    "\n",
    "\n",
    "ax.set_yscale('log')\n",
    "plt.xlabel('days since', color='w')\n",
    "plt.xticks(color='w')\n",
    "plt.xlim(60,440)\n",
    "plt.ylabel('New Cases per Million', fontsize=12)\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('New and Total COVID-19 Cases per Million in GBR, USA and India', fontsize=15)\n",
    "\n",
    "ax1 = df_GBR.plot(y= 'total_cases_per_million_smoothed', x ='DaysSince1Jan20', figsize=(15,5), label='GBR', color='k')\n",
    "df_IND.plot(y= 'total_cases_per_million_smoothed', x ='DaysSince1Jan20', ax=ax1, label = 'IND', color = '#06b202')\n",
    "df_USA.plot(y= 'total_cases_per_million_smoothed', x ='DaysSince1Jan20', ax=ax1, label = 'USA', color ='r')\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "plt.vlines(x=[83,305,371], ymin=0, ymax=100000, color='#85827e', alpha=0.7)\n",
    "plt.text(83.2,0.001,'2020-03-23', color='#85827e', alpha=0.8)\n",
    "plt.text(305.2,0.001,'2020-11-01', color='#85827e',alpha=0.8)\n",
    "plt.text(371.2,0.001, '2020-01-05',color='#85827e',alpha=0.8)\n",
    "\n",
    "ax1.set_yscale('log')\n",
    "plt.ylabel('Total Cases per Million', fontsize=12)\n",
    "plt.xlabel('Days since 2020-01-01', fontsize=12)\n",
    "plt.xlim(60,440)\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3M\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(r'/Users/oscargilmour/Downloads/owid-covid-data.csv')\n",
    "df1=df[['iso_code','new_cases_per_million','human_development_index']]\n",
    "df2=df[['iso_code','new_cases_per_million','gdp_per_capita']]\n",
    "mean = df.groupby(['iso_code','continent'])['new_cases_per_million','human_development_index'].mean()\n",
    "mean2 = df.groupby(['iso_code','continent'])['new_cases_per_million','gdp_per_capita'].mean()\n",
    "meand=mean.dropna()\n",
    "meand2=mean2.dropna()\n",
    "meand.dropna()\n",
    "\n",
    "colors = {'North America':'red', 'Europe':'green', 'Asia':'blue', 'Oceania':'yellow', 'South America':'orange', 'Africa':'purple'}\n",
    "\n",
    "\n",
    "meand.plot(x=['human_development_index'],y=['new_cases_per_million'], kind='scatter', figsize=(10,5), s=30, alpha=0.6)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Average New Cases Per Million')\n",
    "plt.xlabel('Human Development Index')\n",
    "plt.title(\"Comparing the Effect of Weath and Developemnt on Combating New COVID-19 Cases\")\n",
    "\n",
    "\n",
    "meand2.plot(x=['gdp_per_capita'],y=['new_cases_per_million'], kind='scatter', figsize=(10,5), s=30,alpha=0.6,c='g')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Average New Cases per Million')\n",
    "plt.xlabel('GDP per Capita')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two graphs show surprisingly that wealthier and more developed countries have higher average new cases. This is porbably down to that in poorer, less developed countries they don't have the capacity or funds to accurately measure COVID-19 cases and there would be a lot more than suggested in the data set, they just weren't able to be recoreded. Also richer countries have far more people traveling through them for business which would increase transmission rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4A\n",
    "Other, it can affect a person but by itself don’t increase violence "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4B\n",
    "Whilst I believe that long exposure to violent video games can increase aggression short term and possibly desensitize people towards violence especially when it is younger people playing them as their brains are still going through cognitive develop and long exposure to violence in video games will incorporate itself into the brain. I however think that violent videos themselves cannot be viewed as contributing factor to increasing violent tendencies as there are much bigger influences that lead to violence such as childhood experiences and socio-economic status."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4C\n",
    "Potential proxies for playing violent video games could be looking at server traffic at specific times of the day, for example looking at server traffic throughout the week at different times could be a measure as if there spikes in traffic from 4-6pm, it could be concluded that these are the younger players playing after school.  Also, although very simplistic you could look at sales of violent video games as from sales you could see how popular a game is.\n",
    "\n",
    "Potential proxies for violence could be looking at cases of violent crime per 1000 or could be looking at how often a gamer is reported, violent outburst on game’s voice chat with other players can lead to a player being reported and it could be extrapolated that games with higher reporting rates are because of players having more violent outbursts\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4D\n",
    "I would run a digital lab experiment into whether violent video games lead to more violent responses with the experiment incorporating gamification and randomized controlled trial to observe behavior. \n",
    "\n",
    "To recruit participants, I would partner with a research company to get a large range of participants so that the results aren’t skewed by local trends. Participants would then receive a link so they can download a demo of a video game to play for 30-60mins. I would have 3 groups with the first group as a control group who play a non-violent video game such as a car racing game, the second group would play a violent video game that is rated 18 such as Call of Duty where it is a first-person shooter with graphic/realistic violence and then the third group would play a video game rated PG-13 like Fortnite where there is violence in it but much less graphic.\n",
    "\n",
    "After playing the certain game, participants will be then asked a set of questions relating to certain hypothetical scenarios and be asked about how they would respond to each scenario with them only being able to choose from responses given. For example, the first scenario could be ‘someone deliberately pushes you during lunchtime at school’ and the options for response:\n",
    "a) walk away\n",
    "b) push them back \n",
    "c) start a fight\n",
    "d) talk to a teacher about it\n",
    "\n",
    "There would be a time limit of 5 seconds to answer the question as I want the participants to give their impulsive response. The responses are designed to have violent and non-violent resolutions to the problem and then what answer the participant picks could show their underlying preference towards violence and we could then study if playing violent v non-violent video games beforehand leads to a bias towards violent response. The data collected won’t give an accurate correlation between violent games and violence but will help develop our psychological understanding on how violent games affects the brain’s response to situations. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4E\n",
    "Ethically there are some barriers to my experiment as it works best with younger participants as they are much more susceptible to the effects of the video game as older people are more cemented in their moral compass regarding violence and would be much less influenced by violent stimuli so my target age group would be 13–18-year old’s. To make sure my experiment is ethical I would get consent from both parents of the child to make sure they are okay with their child being exposed to violent video games as some parents don’t mind, but other parents may. Along with parental consent, to keep anonymity I would only record age and postcode as other personal information is irrelevant to my question. The final ethical concern is that if I am wrong and violent video games can cause violence, then I’m exposing a group of teenagers to this, to try and combat this there would be an opt out policy where participants can leave whenever they like and at the end there will be a debriefing on how violence is wrong."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4F\n",
    "The proposed study seems to have high statistical conclusion validity as from the study there is no reason to doubt they haven’t computed their data correctly\n",
    "\n",
    "Internal validity center around if the procedures were correctly done such as randomization and measuring of outcome. Looking at just the individual city means the study is less random as data could be skewed by any underlying regional trends. Also, in the measurement of outcome they may not have accurately measured violent video game sales as a lot of games are bought through online shops such as Xbox store or Steam and therefore misses this data when collecting sales. Also data from the neighborhood watch could be incomplete and be different official police data.\n",
    "\n",
    "When looking at construct validity, how well the data matches the theory could be an issue that arises because of doing it in a big city, the underlying population size would have a much bigger impact as neighborhoods with bigger populations probably have higher crime rates and violent game sales but as a spurious correlation rather than causal with population size being the main driver.\n",
    "\n",
    "External validity asks if the study can be generalized, in this case conducting this experiment in the single city of London raises concerns about its external validity as different cities have different factors driving their violent crime rates. For example, if this was conducted in Cambridge where there is probably less violent crime they would observe a different conclusion.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
